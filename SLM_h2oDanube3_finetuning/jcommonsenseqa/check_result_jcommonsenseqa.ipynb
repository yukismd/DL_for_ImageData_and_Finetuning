{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19532975-b992-440a-9a25-86b2e7d9d8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f9d8f8-51fc-4cd0-a5ac-c96773f86693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62fc86b9-b096-4358-a2a8-b1e6a0af76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_result_jcommonsenseqa.ipynb \u001b[34mflexeval_jcommonsenseqa_yukismd\u001b[m\u001b[m\n",
      "\u001b[34mflexeval_jcommonsenseqa\u001b[m\u001b[m           readme.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2063965-7ca1-4aa5-818b-b5e6bb2bd062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lm_prompt</th>\n",
       "      <th>lm_output</th>\n",
       "      <th>task_inputs</th>\n",
       "      <th>references</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「川蜘蛛」\\n1.「足長蜘蛛」\\n2.「アヘアヘ」\\n3...</td>\n",
       "      <td>マザーボード</td>\n",
       "      <td>{'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回...</td>\n",
       "      <td>[マザーボード]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「スーツ」\\n1.「ニュージーランド」\\n2.「勉強」\\...</td>\n",
       "      <td>田園</td>\n",
       "      <td>{'q_id': 8940, 'question': '田んぼが広がる風景を何という？', ...</td>\n",
       "      <td>[田園]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「イラン」\\n1.「カナダ」\\n2.「アメリカ」\\n3....</td>\n",
       "      <td>起きる</td>\n",
       "      <td>{'q_id': 8941, 'question': 'しゃがんだりする様を何という？', ...</td>\n",
       "      <td>[腰を下す]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「テーブルクロス」\\n1.「洗濯」\\n2.「カーテン」\\...</td>\n",
       "      <td>流し</td>\n",
       "      <td>{'q_id': 8942, 'question': '水を出すときに捻るものは？', 'c...</td>\n",
       "      <td>[蛇口]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「水筒」\\n1.「精を出す」\\n2.「魚を獲る」\\n3....</td>\n",
       "      <td>ショッピングセンター</td>\n",
       "      <td>{'q_id': 8943, 'question': '音楽にあわせて踊る場所は？', 'c...</td>\n",
       "      <td>[ディスコ]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           lm_prompt   lm_output  \\\n",
       "0  正しい答えは何でしょう？\\n\\n0.「川蜘蛛」\\n1.「足長蜘蛛」\\n2.「アヘアヘ」\\n3...      マザーボード   \n",
       "1  正しい答えは何でしょう？\\n\\n0.「スーツ」\\n1.「ニュージーランド」\\n2.「勉強」\\...          田園   \n",
       "2  正しい答えは何でしょう？\\n\\n0.「イラン」\\n1.「カナダ」\\n2.「アメリカ」\\n3....         起きる   \n",
       "3  正しい答えは何でしょう？\\n\\n0.「テーブルクロス」\\n1.「洗濯」\\n2.「カーテン」\\...          流し   \n",
       "4  正しい答えは何でしょう？\\n\\n0.「水筒」\\n1.「精を出す」\\n2.「魚を獲る」\\n3....  ショッピングセンター   \n",
       "\n",
       "                                         task_inputs references  exact_match  \n",
       "0  {'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回...   [マザーボード]         True  \n",
       "1  {'q_id': 8940, 'question': '田んぼが広がる風景を何という？', ...       [田園]         True  \n",
       "2  {'q_id': 8941, 'question': 'しゃがんだりする様を何という？', ...     [腰を下す]        False  \n",
       "3  {'q_id': 8942, 'question': '水を出すときに捻るものは？', 'c...       [蛇口]        False  \n",
       "4  {'q_id': 8943, 'question': '音楽にあわせて踊る場所は？', 'c...     [ディスコ]        False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res_original = pd.read_json('flexeval_jcommonsenseqa/outputs.jsonl', orient='records', lines=True)\n",
    "print(df_res_original.shape)\n",
    "df_res_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5967e9-7a21-4b90-9f3a-30f3a471b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_original = df_res_original.rename(columns={'lm_output':'lm_output_original', 'exact_match':'exact_match_original'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2fa3e8-daba-43a5-b3ac-0794886ef3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lm_prompt</th>\n",
       "      <th>lm_output</th>\n",
       "      <th>task_inputs</th>\n",
       "      <th>references</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「川蜘蛛」\\n1.「足長蜘蛛」\\n2.「アヘアヘ」\\n3...</td>\n",
       "      <td>マザーボード</td>\n",
       "      <td>{'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回...</td>\n",
       "      <td>[マザーボード]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「スーツ」\\n1.「ニュージーランド」\\n2.「勉強」\\...</td>\n",
       "      <td>田園</td>\n",
       "      <td>{'q_id': 8940, 'question': '田んぼが広がる風景を何という？', ...</td>\n",
       "      <td>[田園]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「イラン」\\n1.「カナダ」\\n2.「アメリカ」\\n3....</td>\n",
       "      <td>寝る</td>\n",
       "      <td>{'q_id': 8941, 'question': 'しゃがんだりする様を何という？', ...</td>\n",
       "      <td>[腰を下す]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「テーブルクロス」\\n1.「洗濯」\\n2.「カーテン」\\...</td>\n",
       "      <td>流し</td>\n",
       "      <td>{'q_id': 8942, 'question': '水を出すときに捻るものは？', 'c...</td>\n",
       "      <td>[蛇口]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「水筒」\\n1.「精を出す」\\n2.「魚を獲る」\\n3....</td>\n",
       "      <td>ディスコ</td>\n",
       "      <td>{'q_id': 8943, 'question': '音楽にあわせて踊る場所は？', 'c...</td>\n",
       "      <td>[ディスコ]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           lm_prompt lm_output  \\\n",
       "0  正しい答えは何でしょう？\\n\\n0.「川蜘蛛」\\n1.「足長蜘蛛」\\n2.「アヘアヘ」\\n3...    マザーボード   \n",
       "1  正しい答えは何でしょう？\\n\\n0.「スーツ」\\n1.「ニュージーランド」\\n2.「勉強」\\...        田園   \n",
       "2  正しい答えは何でしょう？\\n\\n0.「イラン」\\n1.「カナダ」\\n2.「アメリカ」\\n3....        寝る   \n",
       "3  正しい答えは何でしょう？\\n\\n0.「テーブルクロス」\\n1.「洗濯」\\n2.「カーテン」\\...        流し   \n",
       "4  正しい答えは何でしょう？\\n\\n0.「水筒」\\n1.「精を出す」\\n2.「魚を獲る」\\n3....      ディスコ   \n",
       "\n",
       "                                         task_inputs references  exact_match  \n",
       "0  {'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回...   [マザーボード]         True  \n",
       "1  {'q_id': 8940, 'question': '田んぼが広がる風景を何という？', ...       [田園]         True  \n",
       "2  {'q_id': 8941, 'question': 'しゃがんだりする様を何という？', ...     [腰を下す]        False  \n",
       "3  {'q_id': 8942, 'question': '水を出すときに捻るものは？', 'c...       [蛇口]        False  \n",
       "4  {'q_id': 8943, 'question': '音楽にあわせて踊る場所は？', 'c...     [ディスコ]         True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res_ft = pd.read_json('flexeval_jcommonsenseqa_yukismd/outputs.jsonl', orient='records', lines=True)\n",
    "print(df_res_ft.shape)\n",
    "df_res_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c2020a-01e6-4088-bb0a-89ef39f259de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_ft = df_res_ft.rename(columns={'lm_output':'lm_output_finetuned', 'exact_match':'exact_match_finetuned'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e934b-21cd-4547-b3c8-d6ce48968e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1275200-22dd-4548-90f9-62e20e8448ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正しい答えは何でしょう？\n",
      "\n",
      "0.「川蜘蛛」\n",
      "1.「足長蜘蛛」\n",
      "2.「アヘアヘ」\n",
      "3.「子守蜘蛛」\n",
      "4.「アメマ」\n",
      "問題：アメンボの別名は？\n",
      "回答：「川蜘蛛」\n",
      "\n",
      "0.「ファスナー」\n",
      "1.「ファンデ」\n",
      "2.「後ろ指」\n",
      "3.「後ろ足」\n",
      "4.「後ろ身頃」\n",
      "問題：衣服の背中を覆う部分のことは？\n",
      "回答：「後ろ身頃」\n",
      "\n",
      "0.「掲示板」\n",
      "1.「パソコン」\n",
      "2.「マザーボード」\n",
      "3.「ハードディスク」\n",
      "4.「まな板」\n",
      "問題：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\n",
      "回答：「\n"
     ]
    }
   ],
   "source": [
    "print(df_res_original['lm_prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23eefb27-5e85-48a3-bea1-42989ae4dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回路基板の事をなんと言う？', 'choice0': '掲示板', 'choice1': 'パソコン', 'choice2': 'マザーボード', 'choice3': 'ハードディスク', 'choice4': 'まな板', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "print(df_res_original['task_inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76aabae-1fcb-45d5-8c6d-b29278543cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "マザーボード\n"
     ]
    }
   ],
   "source": [
    "print(df_res_original['lm_output_original'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dac1ee60-8e07-4cdc-b687-130b2b9ea966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正しい答えは何でしょう？\n",
      "\n",
      "0.「スーツ」\n",
      "1.「ニュージーランド」\n",
      "2.「勉強」\n",
      "3.「上着」\n",
      "4.「ワイシャツ」\n",
      "問題：子どもから大人まで様々な立場の人が日常的に着るものはどれ？\n",
      "回答：「上着」\n",
      "\n",
      "0.「ネコ」\n",
      "1.「ディズニーランド」\n",
      "2.「イヌ」\n",
      "3.「玄関口」\n",
      "4.「モーテル」\n",
      "問題：家族が待っている、ホッと一息つける安心への入り口てなあに？\n",
      "回答：「玄関口」\n",
      "\n",
      "0.「畑」\n",
      "1.「海」\n",
      "2.「田園」\n",
      "3.「地方」\n",
      "4.「牧場」\n",
      "問題：田んぼが広がる風景を何という？\n",
      "回答：「\n"
     ]
    }
   ],
   "source": [
    "print(df_res_original['lm_prompt'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec7843-128a-4205-8b48-74e5fe59d2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b615319b-88a5-4fe3-b992-5da6f84c0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lm_prompt</th>\n",
       "      <th>task_inputs</th>\n",
       "      <th>references</th>\n",
       "      <th>lm_output_original</th>\n",
       "      <th>exact_match_original</th>\n",
       "      <th>lm_output_finetuned</th>\n",
       "      <th>exact_match_finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「川蜘蛛」\\n1.「足長蜘蛛」\\n2.「アヘアヘ」\\n3...</td>\n",
       "      <td>{'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回...</td>\n",
       "      <td>[マザーボード]</td>\n",
       "      <td>マザーボード</td>\n",
       "      <td>True</td>\n",
       "      <td>マザーボード</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「スーツ」\\n1.「ニュージーランド」\\n2.「勉強」\\...</td>\n",
       "      <td>{'q_id': 8940, 'question': '田んぼが広がる風景を何という？', ...</td>\n",
       "      <td>[田園]</td>\n",
       "      <td>田園</td>\n",
       "      <td>True</td>\n",
       "      <td>田園</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「イラン」\\n1.「カナダ」\\n2.「アメリカ」\\n3....</td>\n",
       "      <td>{'q_id': 8941, 'question': 'しゃがんだりする様を何という？', ...</td>\n",
       "      <td>[腰を下す]</td>\n",
       "      <td>起きる</td>\n",
       "      <td>False</td>\n",
       "      <td>寝る</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「テーブルクロス」\\n1.「洗濯」\\n2.「カーテン」\\...</td>\n",
       "      <td>{'q_id': 8942, 'question': '水を出すときに捻るものは？', 'c...</td>\n",
       "      <td>[蛇口]</td>\n",
       "      <td>流し</td>\n",
       "      <td>False</td>\n",
       "      <td>流し</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>正しい答えは何でしょう？\\n\\n0.「水筒」\\n1.「精を出す」\\n2.「魚を獲る」\\n3....</td>\n",
       "      <td>{'q_id': 8943, 'question': '音楽にあわせて踊る場所は？', 'c...</td>\n",
       "      <td>[ディスコ]</td>\n",
       "      <td>ショッピングセンター</td>\n",
       "      <td>False</td>\n",
       "      <td>ディスコ</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           lm_prompt  \\\n",
       "0  正しい答えは何でしょう？\\n\\n0.「川蜘蛛」\\n1.「足長蜘蛛」\\n2.「アヘアヘ」\\n3...   \n",
       "1  正しい答えは何でしょう？\\n\\n0.「スーツ」\\n1.「ニュージーランド」\\n2.「勉強」\\...   \n",
       "2  正しい答えは何でしょう？\\n\\n0.「イラン」\\n1.「カナダ」\\n2.「アメリカ」\\n3....   \n",
       "3  正しい答えは何でしょう？\\n\\n0.「テーブルクロス」\\n1.「洗濯」\\n2.「カーテン」\\...   \n",
       "4  正しい答えは何でしょう？\\n\\n0.「水筒」\\n1.「精を出す」\\n2.「魚を獲る」\\n3....   \n",
       "\n",
       "                                         task_inputs references  \\\n",
       "0  {'q_id': 8939, 'question': '電子機器で使用される最も主要な電子回...   [マザーボード]   \n",
       "1  {'q_id': 8940, 'question': '田んぼが広がる風景を何という？', ...       [田園]   \n",
       "2  {'q_id': 8941, 'question': 'しゃがんだりする様を何という？', ...     [腰を下す]   \n",
       "3  {'q_id': 8942, 'question': '水を出すときに捻るものは？', 'c...       [蛇口]   \n",
       "4  {'q_id': 8943, 'question': '音楽にあわせて踊る場所は？', 'c...     [ディスコ]   \n",
       "\n",
       "  lm_output_original  exact_match_original lm_output_finetuned  \\\n",
       "0             マザーボード                  True              マザーボード   \n",
       "1                 田園                  True                  田園   \n",
       "2                起きる                 False                  寝る   \n",
       "3                 流し                 False                  流し   \n",
       "4         ショッピングセンター                 False                ディスコ   \n",
       "\n",
       "   exact_match_finetuned  \n",
       "0                   True  \n",
       "1                   True  \n",
       "2                  False  \n",
       "3                  False  \n",
       "4                   True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.DataFrame({'lm_prompt':df_res_original['lm_prompt'],\n",
    "                      'task_inputs':df_res_original['task_inputs'],\n",
    "                      'references':df_res_original['references'],\n",
    "                      'lm_output_original':df_res_original['lm_output_original'],\n",
    "                      'exact_match_original':df_res_original['exact_match_original'],\n",
    "                      'lm_output_finetuned':df_res_ft['lm_output_finetuned'],\n",
    "                      'exact_match_finetuned':df_res_ft['exact_match_finetuned']})\n",
    "print(df_res.shape)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c0cace-bb48-4721-b21b-32c5be2f359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv('results_commonsenseqa.csv', index=False)\n",
    "#df_res.to_csv('results_commonsenseqa_SJIS.csv', index=False, encoding='sjis', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a54ab-73a9-4d43-99d6-290eaf236720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d598e1a1-bbb8-451b-9144-b17bcd8e2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exact_match_original\n",
       "True                    0.576408\n",
       "False                   0.423592\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[['exact_match_original']].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4617be0a-1b22-4479-8723-1227e96a2d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exact_match_finetuned\n",
       "True                     0.513852\n",
       "False                    0.486148\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[['exact_match_finetuned']].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce61ab-8a4e-446f-8761-7b08ca691e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
