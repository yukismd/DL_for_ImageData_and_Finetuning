## モデルのチューニング戦略

### 画像分類/回帰 - Kaggle Grand Masterに教えてもらった経験的な教訓
- 影響の大きいハイパーパラメータ（チューニング対象）
    - Backbone
    - 画像サイズ
    - 学習率（通常、Backbornや画像サイズと交互作用あり）
    - エポック数（学習率が小さい場合、ドロップアウトが大きい場合、複雑なデータ拡張の実施、といった場合多くのエポック数が必要）
    - データ拡張
- バッチサイズも影響が考えられるが、32~256の範囲では一般的に大きな影響がないと考えられる。小さいとBatch Normalizationが安定しなくなる。学習環境のメモリ（GPU）に合う範囲で調整
- 多くの実験を繰り返すため、比較的汎用的で小型のbackboneを選択し、比較的小さく画像サイズを設定し、学習環境のメモリ合わせてバッチサイズを固定。残りのハイパーパラメータ（学習率、エポック数、データ拡張）のチューニング間は固定。最終段階でモデルサイズと画像サイズのスケールアップを試みる。（スケールアップの際、学習率の再調整の必要あり）
- BackbornはEfficientNetやResNetが汎用的で良い
- 優先度の高いデータ拡張は、horizontal、Vertical、Transpose Flip（３つの反転）。それらの次は、Shift、Scale、Rotate。HUE、Saturation、Random Noise、Blurなども試してみる価値あり。（データ拡張を多用している場合、エポック数が十分か確認）

### 画像距離学習/Image Metric Learning/Image Similarity - Kaggleに挑む深層学習プログラミングの極意（講談社）
注：ArcFaceアプローチを仮定
- 距離学習特有のハイパーパラメータ。これらのハイパーパラメータは、基本的にBackbornに依存しない
    - ArcFaceマージンペナルティ（m）: 0.2,0.4,0.6,0.8,1.0辺り
    - ArcFaceスケーリング（s）: 20,32,40,48,64辺り
    - 画像Embedding次元: 512程度で十分だが、余裕があれば1024や1792も試す 
- チューニング戦略として、小さなBackboneと画像サイズで、距離学習特有のハイパーパラメータやデータ拡張を検証（これらはタスクやデータに依存して決まることが多い）。その後、大きなBackboneや画像サイズのスケールアップを検証
- 画像距離学習では、GeM Pooling(Generalized Mean Pooling)がよく利用される

### テキスト分類/回帰 - Kaggleに挑む深層学習プログラミングの極意（講談社）
- BERTの他によく使われるBERT系アーキテクチャ [p.177]
    - RoBERTa
    - ELECTRA
    - DeBERTa
- BERTでよく持ちいられるOptimizerはAdamW [p.183]
- バッチサイズ、学習率、エポック数の影響は大きい。以下の範囲での探索を実施する場合が多い [p.184]
    - バッチサイズ: 16,32
    - 学習率: 5e-05,3e-05,2e-5
    - エポック数: 2,3,4
- 自動混合精度（Automatic Mixed Precision）を用いることにより、学習の高速化が可能 [p.186]
- バッチサイズが小さくなってしまう場合は、学習を安定させるために勾配累積（Gradient Accumulation）が有用 [p.188]
- ファインチューニングの不安定性とその対処に関して - [Revisiting Few-sample BERT Fine-tuning](https://arxiv.org/abs/2006.05987) [p.193]
- 取り組むタスクに似たタスクで事前学習が実施された事前学習モデルを選択すると良い [p.196]
- テキストの入力方法の工夫 [p.197]
- 外部データの活用 [p.197]

