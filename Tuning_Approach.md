## モデルのチューニング戦略

### 画像分類/回帰 - Kaggle Grand Masterに教えてもらった経験的な教訓
- 影響の大きいハイパーパラメータ（チューニング対象）
    - Backbone
    - 画像サイズ
    - 学習率（通常、Backbornや画像サイズと交互作用あり）
    - エポック数（学習率が小さい場合、ドロップアウトが大きい場合、複雑なデータ拡張の実施、といった場合多くのエポック数が必要）
    - データ拡張
- バッチサイズも影響が考えられるが、32~256の範囲では一般的に大きな影響がないと考えられる。小さいとBatch Normalizationが安定しなくなる。学習環境のメモリ（GPU）に合う範囲で調整
- 多くの実験を繰り返すため、比較的汎用的で小型のbackboneを選択し、比較的小さく画像サイズを設定し、学習環境のメモリ合わせてバッチサイズを固定。残りのハイパーパラメータ（学習率、エポック数、データ拡張）のチューニング間は固定。最終段階でモデルサイズと画像サイズのスケールアップを試みる。（スケールアップの際、学習率の再調整の必要あり）
- BackbornはEfficientNetやResNetが汎用的で良い
- 優先度の高いデータ拡張は、horizontal、Vertical、Transpose Flip（３つの反転）。それらの次は、Shift、Scale、Rotate。HUE、Saturation、Random Noise、Blurなども試してみる価値あり。（データ拡張を多用している場合、エポック数が十分か確認）

### テキスト分類/回帰 - Kaggleに挑む深層学習プログラミングの極意（講談社）
- BERTでよく持ちいられるOptimizerはAdamW [p.183]
    - Backbone
- バッチサイズ、学習率、エポック数の影響は大きい。以下の範囲での探索を実施する場合が多い [p.184]
    - バッチサイズ: 16,32
    - 学習率: 5e-05,3e-05,2e-5
    - エポック数: 2,3,4
- 自動混合精度（Automatic Mixed Precision）を用いることにより、学習の高速化が可能 [p.186]
- バッチサイズが小さくなってしまう場合は、学習を安定させるために勾配累積（Gradient Accumulation）が有用 [p.188]
- ファインチューニングの不安定性とその対処に関して - [Revisiting Few-sample BERT Fine-tuning](https://arxiv.org/abs/2006.05987) [p.193]


